# import os
# import tensorflow as tf
# import numpy as np
# from datasets import load_data  # ä½ ä¹‹å‰å¯«çš„ tf.data.Dataset å°è£
# from tensorflow.keras.losses import MeanSquaredError

# # è©•ä¼°ä¸€å€‹æ¨¡å‹ï¼Œå›å‚³ MSE
# def evaluate_model(model_path, val_dataset):
#     model = tf.keras.models.load_model(model_path)
#     mse_loss = MeanSquaredError()

#     total_loss = 0.0
#     count = 0

#     for batch in val_dataset:
#         images, sensor_data = batch
#         preds = model(images, training=False)
#         loss = mse_loss(sensor_data, preds).numpy()
#         total_loss += loss * images.shape[0]
#         count += images.shape[0]

#     avg_mse = total_loss / count
#     return avg_mse

# # ä¸»æµç¨‹
# def evaluate_all_models(model_dir='./checkpoints', val_txt='./data/val.txt', batch_size=64):
#     # æº–å‚™é©—è­‰è³‡æ–™
#     val_dataset = load_data(val_txt).batch(batch_size).prefetch(tf.data.AUTOTUNE)

#     # æ‰¾å‡ºæ‰€æœ‰æ¨¡å‹
#     model_files = sorted([f for f in os.listdir(model_dir) if f.endswith('.h5')])
#     best_mse = float('inf')
#     best_model = None

#     print("ğŸ” é–‹å§‹é©—è­‰æ‰€æœ‰æ¨¡å‹...\n")
#     for model_file in model_files:
#         model_path = os.path.join(model_dir, model_file)
#         mse = evaluate_model(model_path, val_dataset)
#         print(f"ğŸ“Š {model_file} â†’ MSE: {mse:.4f}")

#         if mse < best_mse:
#             best_mse = mse
#             best_model = model_file

#     print("\nâœ… æœ€ä½³æ¨¡å‹ï¼š", best_model)
#     print(f"ğŸ¯ æœ€ä½ MSEï¼š{best_mse:.4f}")

# if __name__ == '__main__':
#     evaluate_all_models()
